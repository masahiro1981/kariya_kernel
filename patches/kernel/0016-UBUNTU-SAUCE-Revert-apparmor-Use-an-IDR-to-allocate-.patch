From 4d7930abecde798b7977a3d58bb0fbab2340bdbd Mon Sep 17 00:00:00 2001
From: John Johansen <john.johansen@canonical.com>
Date: Thu, 14 Mar 2019 15:37:32 -0700
Subject: [PATCH 16/31] UBUNTU: SAUCE: Revert "apparmor: Use an IDR to allocate
 apparmor secids"

Revert currently unused by Ubuntu apparmor features that require secid
support, to enable apparmor LSM stacking with the 5.1 LSM stacking
patchset.

This reverts commit 99cc45e486786c7215a7e39824c3bbaf7cf2fc08.

Signed-off-by: John Johansen <john.johansen@canonical.com>
Signed-off-by: Seth Forshee <seth.forshee@canonical.com>
---
 security/apparmor/secid.c | 114 ++++++++++++++++++++++++++++++++++----
 1 file changed, 103 insertions(+), 11 deletions(-)

diff --git a/security/apparmor/secid.c b/security/apparmor/secid.c
index 9d5aae368187..41657e7609d3 100644
--- a/security/apparmor/secid.c
+++ b/security/apparmor/secid.c
@@ -13,7 +13,6 @@
 #include <linux/errno.h>
 #include <linux/err.h>
 #include <linux/gfp.h>
-#include <linux/idr.h>
 #include <linux/slab.h>
 #include <linux/spinlock.h>
 
@@ -26,10 +25,18 @@
 /*
  * secids - do not pin labels with a refcount. They rely on the label
  * properly updating/freeing them
+ *
+ * A singly linked free list is used to track secids that have been
+ * freed and reuse them before allocating new ones
  */
 
-static DEFINE_IDR(aa_secids);
+#define FREE_LIST_HEAD 1
+
+static RADIX_TREE(aa_secids_map, GFP_ATOMIC);
 static DEFINE_SPINLOCK(secid_lock);
+static u32 alloced_secid = FREE_LIST_HEAD;
+static u32 free_list = FREE_LIST_HEAD;
+static unsigned long free_count;
 
 /*
  * TODO: allow policy to reserve a secid range?
@@ -37,6 +44,65 @@ static DEFINE_SPINLOCK(secid_lock);
  * TODO: use secid_update in label replace
  */
 
+#define SECID_MAX U32_MAX
+
+/* TODO: mark free list as exceptional */
+static void *to_ptr(u32 secid)
+{
+	return (void *)
+		((((unsigned long) secid) << RADIX_TREE_EXCEPTIONAL_SHIFT));
+}
+
+static u32 to_secid(void *ptr)
+{
+	return (u32) (((unsigned long) ptr) >> RADIX_TREE_EXCEPTIONAL_SHIFT);
+}
+
+
+/* TODO: tag free_list entries to mark them as different */
+static u32 __pop(struct aa_label *label)
+{
+	u32 secid = free_list;
+	void __rcu **slot;
+	void *entry;
+
+	if (free_list == FREE_LIST_HEAD)
+		return AA_SECID_INVALID;
+
+	slot = radix_tree_lookup_slot(&aa_secids_map, secid);
+	AA_BUG(!slot);
+	entry = radix_tree_deref_slot_protected(slot, &secid_lock);
+	free_list = to_secid(entry);
+	radix_tree_replace_slot(&aa_secids_map, slot, label);
+	free_count--;
+
+	return secid;
+}
+
+static void __push(u32 secid)
+{
+	void __rcu **slot;
+
+	slot = radix_tree_lookup_slot(&aa_secids_map, secid);
+	AA_BUG(!slot);
+	radix_tree_replace_slot(&aa_secids_map, slot, to_ptr(free_list));
+	free_list = secid;
+	free_count++;
+}
+
+static struct aa_label * __secid_update(u32 secid, struct aa_label *label)
+{
+	struct aa_label *old;
+	void __rcu **slot;
+
+	slot = radix_tree_lookup_slot(&aa_secids_map, secid);
+	AA_BUG(!slot);
+	old = radix_tree_deref_slot_protected(slot, &secid_lock);
+	radix_tree_replace_slot(&aa_secids_map, slot, label);
+
+	return old;
+}
+
 /**
  * aa_secid_update - update a secid mapping to a new label
  * @secid: secid to update
@@ -44,10 +110,11 @@ static DEFINE_SPINLOCK(secid_lock);
  */
 void aa_secid_update(u32 secid, struct aa_label *label)
 {
+	struct aa_label *old;
 	unsigned long flags;
 
 	spin_lock_irqsave(&secid_lock, flags);
-	idr_replace(&aa_secids, label, secid);
+	old = __secid_update(secid, label);
 	spin_unlock_irqrestore(&secid_lock, flags);
 }
 
@@ -60,7 +127,7 @@ struct aa_label *aa_secid_to_label(u32 secid)
 	struct aa_label *label;
 
 	rcu_read_lock();
-	label = idr_find(&aa_secids, secid);
+	label = radix_tree_lookup(&aa_secids_map, secid);
 	rcu_read_unlock();
 
 	return label;
@@ -95,6 +162,7 @@ int apparmor_secid_to_secctx(u32 secid, char **secdata, u32 *seclen)
 	return 0;
 }
 
+
 int apparmor_secctx_to_secid(const char *secdata, u32 seclen, u32 *secid)
 {
 	struct aa_label *label;
@@ -113,6 +181,7 @@ void apparmor_release_secctx(char *secdata, u32 seclen)
 	kfree(secdata);
 }
 
+
 /**
  * aa_alloc_secid - allocate a new secid for a profile
  */
@@ -121,12 +190,35 @@ u32 aa_alloc_secid(struct aa_label *label, gfp_t gfp)
 	unsigned long flags;
 	u32 secid;
 
-	idr_preload(gfp);
-	spin_lock_irqsave(&secid_lock, flags);
-	secid = idr_alloc(&aa_secids, label, 0, 0, GFP_ATOMIC);
-	/* XXX: Can return -ENOMEM */
-	spin_unlock_irqrestore(&secid_lock, flags);
-	idr_preload_end();
+	/* racey, but at worst causes new allocation instead of reuse */
+	if (free_list == FREE_LIST_HEAD) {
+		bool preload = 0;
+		int res;
+
+retry:
+		if (gfpflags_allow_blocking(gfp) && !radix_tree_preload(gfp))
+			preload = 1;
+		spin_lock_irqsave(&secid_lock, flags);
+		if (alloced_secid != SECID_MAX) {
+			secid = ++alloced_secid;
+			res = radix_tree_insert(&aa_secids_map, secid, label);
+			AA_BUG(res == -EEXIST);
+		} else {
+			secid = AA_SECID_INVALID;
+		}
+		spin_unlock_irqrestore(&secid_lock, flags);
+		if (preload)
+			radix_tree_preload_end();
+	} else {
+		spin_lock_irqsave(&secid_lock, flags);
+		/* remove entry from free list */
+		secid = __pop(label);
+		if (secid == AA_SECID_INVALID) {
+			spin_unlock_irqrestore(&secid_lock, flags);
+			goto retry;
+		}
+		spin_unlock_irqrestore(&secid_lock, flags);
+	}
 
 	return secid;
 }
@@ -140,6 +232,6 @@ void aa_free_secid(u32 secid)
 	unsigned long flags;
 
 	spin_lock_irqsave(&secid_lock, flags);
-	idr_remove(&aa_secids, secid);
+	__push(secid);
 	spin_unlock_irqrestore(&secid_lock, flags);
 }
-- 
2.25.1

